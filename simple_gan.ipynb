{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "simple-gan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCi5-rWjsNXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "img_size = 64\n",
        "\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.Resize(img_size),\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.5,),(0.5,))])\n",
        "batch_size = 100\n",
        "\n",
        "# Replace MNIST with KMNIST if so desired.\n",
        "trainset = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhH1ME8GD1WY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# designed for use on Google Colab\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MN1IWpN17O20",
        "colab": {}
      },
      "source": [
        "# constants and flags\n",
        "noise_dim = 100\n",
        "gen_f = 64\n",
        "dis_f = 64\n",
        "nc = 1\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "add_noise = False\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = False\n",
        "md_dim = 1\n",
        "md_k = 2\n",
        "\n",
        "stddev_layer = False\n",
        "\n",
        "label_smooth = False\n",
        "\n",
        "unroll_steps = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp24n2sbsNZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "img, labels = next(iter(trainloader))\n",
        "img = torchvision.utils.make_grid(img, normalize=True)\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(np.transpose(img.numpy(),(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKzav97BsNa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8SCeX-NsNa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG_q3FkHLYGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Minibatch_Disc(nn.Module):\n",
        "    def __init__(self, d_in, d_out, k_size):\n",
        "        super(Minibatch_Disc, self).__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.k_size = k_size\n",
        "    \n",
        "        self.T = nn.Parameter(torch.Tensor(d_in, d_out, k_size))\n",
        "        nn.init.normal_(self.T, 0, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x_vector = x.view(-1, self.d_in)\n",
        "        mat = x_vector.mm(self.T.view(self.d_in, -1))\n",
        "        mat = mat.view(-1, self.d_out, self.k_size).unsqueeze(0)\n",
        "        norm = torch.abs(mat - mat.permute(1,0,2,3)).sum(3)\n",
        "        norm = torch.exp(-norm)\n",
        "        return torch.cat([x_vector, norm.sum(0) - 1], 1).view(x.size()[0], -1, x.size()[2], x.size()[3])\n",
        "\n",
        "class Stddev(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stddev, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        stddev = x.std(dim=0).mean()\n",
        "        return torch.cat([x, stddev.expand(x.size()[0], 1, x.size()[2], x.size()[3])], dim=1)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbrTculysNa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.decon1 = nn.ConvTranspose2d(noise_dim, gen_f*8, 4, 1, 0)\n",
        "        self.norm1 = nn.BatchNorm2d(gen_f*8)\n",
        "        \n",
        "        self.decon2 = nn.ConvTranspose2d(gen_f*8, gen_f*4, 4, 2, 1)\n",
        "        self.norm2 = nn.BatchNorm2d(gen_f*4)\n",
        "        \n",
        "        self.decon3 = nn.ConvTranspose2d(gen_f*4, gen_f*2, 4, 2, 1)\n",
        "        self.norm3 = nn.BatchNorm2d(gen_f*2)\n",
        "        \n",
        "        self.decon4 = nn.ConvTranspose2d(gen_f*2, gen_f, 4, 2, 1)\n",
        "        self.norm4 = nn.BatchNorm2d(gen_f)\n",
        "        \n",
        "        self.decon_final = nn.ConvTranspose2d(gen_f, nc, 4, 2, 1)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        x = F.leaky_relu(self.norm1(self.decon1(input)), 0.2)\n",
        "        x = F.leaky_relu(self.norm2(self.decon2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.norm3(self.decon3(x)), 0.2)\n",
        "        x = F.leaky_relu(self.norm4(self.decon4(x)), 0.2)\n",
        "        x = torch.tanh(self.decon_final(x))\n",
        "        return x\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.std = 0.3\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(nc, dis_f, 4, 2, 1)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(dis_f, dis_f*2, 4, 2, 1)\n",
        "        self.norm2 = nn.BatchNorm2d(dis_f*2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(dis_f*2, dis_f*4, 4, 2, 1)\n",
        "        self.norm3 = nn.BatchNorm2d(dis_f*4)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(dis_f*4, dis_f*8, 4, 2, 1)\n",
        "        self.norm4 = nn.BatchNorm2d(dis_f*8)\n",
        "        \n",
        "        if mini_disc:\n",
        "            self.conv_final = nn.Conv2d(dis_f*8 + md_dim, 1, 4, 1, 0)\n",
        "            self.mbd = Minibatch_Disc(dis_f*8, md_dim, md_k)\n",
        "        elif stddev_layer:\n",
        "            self.stddev = Stddev()\n",
        "            self.conv_final = nn.Conv2d(dis_f*8 + 1, 1, 4, 1, 0)\n",
        "        else:\n",
        "            self.conv_final = nn.Conv2d(dis_f*8, 1, 4, 1, 0)\n",
        "        \n",
        "    def add_noise(self, x, mean, std):\n",
        "        noise = torch.normal(torch.zeros(x.shape, device=device), torch.full(x.shape, std, dtype=torch.float, device=device))\n",
        "        return x + noise\n",
        "\n",
        "    def forward(self, input):\n",
        "        if add_noise:\n",
        "            x = self.add_noise(input, 0, self.std)\n",
        "            self.std *= annealing\n",
        "        else:\n",
        "            x = input\n",
        "\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.norm2(self.conv2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.norm3(self.conv3(x)), 0.2)\n",
        "        x = F.leaky_relu(self.norm4(self.conv4(x)), 0.2)\n",
        "        \n",
        "        if mini_disc:\n",
        "            x = self.mbd(x)\n",
        "        elif stddev_layer:\n",
        "            x = self.stddev(x)\n",
        "        x = torch.sigmoid(self.conv_final(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR_lO54xvVlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self):\n",
        "        self.netG = Generator().to(device)\n",
        "        self.netG.apply(weights_init)\n",
        "\n",
        "        self.netD = Discriminator().to(device)\n",
        "        self.netD.apply(weights_init)\n",
        "\n",
        "        self.optimizerD = optim.Adam(self.netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.optimizerG = optim.Adam(self.netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        \n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "        self.real_label = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
        "        if label_smooth:\n",
        "            self.real_label += torch.normal(torch.zeros(self.real_label.shape, device=device),\n",
        "                              torch.full(self.real_label.shape, 0.1, device=device))\n",
        "        self.fake_label = torch.full((batch_size,), 0, dtype=torch.float, device=device)\n",
        "        # label smoothing is not usually applied to negative labels for mathematical reasons\n",
        "        # by uncommenting the lines below, one may observe why\n",
        "        #if label_smooth:\n",
        "        #    noise = torch.normal(torch.zeros(self.real_label.shape, device=device),\n",
        "        #                      torch.full(self.real_label.shape, 0.2, dtype=torch.float, device=device))\n",
        "        #    self.fake_label += torch.where(noise > 0, noise, torch.zeros(self.real_label.shape, device=device))\n",
        "        \n",
        "    def unroll_loop(self):\n",
        "        self.netD.zero_grad()\n",
        "    \n",
        "        data = next(iter(trainloader))\n",
        "        real = data[0].to(device)\n",
        "        b_size = real.size(0)\n",
        "\n",
        "        output = self.netD(real).view(-1)\n",
        "        errD_real = self.criterion(output, self.real_label)\n",
        "        errD_real.backward()\n",
        "        D_real = output.mean().item()\n",
        "\n",
        "        noise = torch.randn(batch_size, noise_dim, 1, 1, device=device)\n",
        "        fake = self.netG(noise)\n",
        "        \n",
        "        output = self.netD(fake.detach()).view(-1)\n",
        "        errD_fake = self.criterion(output, self.fake_label)\n",
        "        errD_fake.backward()\n",
        "        D_fake = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        self.optimizerD.step()\n",
        "    \n",
        "        return (D_real, D_fake)\n",
        "    \n",
        "    def train(self):\n",
        "        G_losses = []\n",
        "        D_losses = []\n",
        "        stddev_z = []\n",
        "        stddev_x = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "                # Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "                self.netD.zero_grad()\n",
        "\n",
        "                real = data[0].to(device)\n",
        "                b_size = real.size(0)\n",
        "\n",
        "                output = self.netD(real).view(-1)\n",
        "                errD_real = self.criterion(output, self.real_label)\n",
        "                errD_real.backward()\n",
        "                D_real = output.mean().item()\n",
        "\n",
        "                noise = torch.randn(batch_size, noise_dim, 1, 1, device=device)\n",
        "                fake = self.netG(noise)\n",
        "                stddev_fake = fake.std(dim=0).mean().item()\n",
        "                stddev_real = real.std(dim=0).mean().item()\n",
        "        \n",
        "                output = self.netD(fake.detach()).view(-1)\n",
        "                errD_fake = self.criterion(output, self.fake_label)\n",
        "                errD_fake.backward()\n",
        "                D_fake1 = output.mean().item()\n",
        "                errD = errD_real + errD_fake\n",
        "                self.optimizerD.step()\n",
        "\n",
        "                # Update G network: maximize log(D(G(z)))\n",
        "                self.netG.zero_grad()\n",
        "        \n",
        "                if unroll_steps > 0:\n",
        "                    backup_dict = deepcopy(self.netD).state_dict()\n",
        "                    for step in range(unroll_steps):\n",
        "                        self.unroll_loop()\n",
        "\n",
        "                output = self.netD(fake).view(-1)\n",
        "                # fake labels treated as real for generator cost\n",
        "                errG = self.criterion(output, self.real_label) \n",
        "                errG.backward()\n",
        "                D_fake2 = output.mean().item()\n",
        "                self.optimizerG.step()\n",
        "        \n",
        "                if unroll_steps > 0:\n",
        "                    self.netD.load_state_dict(backup_dict)\n",
        "                    del backup_dict\n",
        "                \n",
        "                D_fake = (D_fake1 + D_fake2) / 2\n",
        "                # Output training stats\n",
        "                if i % 10 == 0:\n",
        "                    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f   std_z: %.4f / std_x: %.4f'\n",
        "                    % (epoch, num_epochs, i, len(trainloader), errD.item(), errG.item(), D_real, D_fake, stddev_fake, stddev_real))\n",
        "\n",
        "                # Save Losses for plotting later\n",
        "                G_losses.append(errG.item())\n",
        "                D_losses.append(errD.item())\n",
        "                stddev_z.append(stddev_fake)\n",
        "                stddev_x.append(stddev_real)\n",
        "        return (G_losses, D_losses, stddev_z, stddev_x)\n",
        "    \n",
        "    def gen_sample(self, noise=None):\n",
        "        if noise == None:\n",
        "            noise = torch.randn(batch_size, noise_dim, 1, 1, device=device)\n",
        "        return self.netG(noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jsM9zjizjmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(G_l, D_l, std_z, std_x):\n",
        "    plt.figure()\n",
        "    plt.plot(G_l,label=\"G\")\n",
        "    plt.plot(D_l,label=\"D\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(std_z, label=\"G(z)\")\n",
        "    plt.plot(std_x, label=\"x\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"stddev\")\n",
        "    plt.legend()\n",
        "\n",
        "def show_sample(img):\n",
        "    img = torchvision.utils.make_grid(img, normalize=True, padding=3)\n",
        "    plt.imshow(np.transpose(img.detach().cpu().numpy(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBp31ZwfX5Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G_losses, D_losses, stddevs_f, stddevs_r = [],[],[],[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DINdU4TUzU2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_noise = False\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = False\n",
        "md_dim = 1\n",
        "md_k = 2\n",
        "\n",
        "stddev_layer = True\n",
        "\n",
        "label_smooth = False\n",
        "\n",
        "unroll_steps = 0\n",
        "\n",
        "for _ in range(25):\n",
        "    vanilla_trainer = Trainer()\n",
        "    v_G_loss, v_D_loss, v_stddev_f, v_stddev_r = vanilla_trainer.train()\n",
        "    plot(v_G_loss, v_D_loss, v_stddev_f, v_stddev_r)\n",
        "\n",
        "    G_losses.append(v_G_loss)\n",
        "    D_losses.append(v_D_loss)\n",
        "    stddevs_f.append(v_stddev_f)\n",
        "    stddevs_r.append(v_stddev_r)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    show_sample(vanilla_trainer.gen_sample())\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXe4KaSJRkdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove suspicious / contaminated data from overall results\n",
        "for i in range(np.asarray(D_losses).shape[0]):\n",
        "    if (np.asarray(D_losses)[i][-1] >= 10 or np.asarray(G_losses)[i][-1] >= 10):\n",
        "        G_l = np.delete(G_losses, i, 0)\n",
        "        D_l = np.delete(D_losses, i, 0)\n",
        "        std_f = np.delete(stddevs_f, i, 0)\n",
        "        std_r = np.delete(stddevs_r, i, 0)\n",
        "\n",
        "G_losses, D_losses, stddevs_f, stddevs_r = G_l, D_l, std_f, std_r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tHIintHZy80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.asarray(G_losses).shape)\n",
        "\n",
        "plot(np.average(G_losses, axis=0), np.average(D_losses, axis=0), np.average(stddevs_f, axis=0), np.average(stddevs_r, axis=0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfMmaNRM_-M0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do train the classifier before using this - it's at the bottom of the page\n",
        "# for reasons of cleaner organisation\n",
        "classifier = torch.load('./classifier')\n",
        "# instert name of trainer to use here\n",
        "trainer = vanilla_trainer\n",
        "\n",
        "# show the first one\n",
        "sample = trainer.gen_sample()\n",
        "show_sample(sample)\n",
        "plt.show()\n",
        "pred = torch.max(classifier(sample), 1)\n",
        "print(pred[1].cpu().numpy())\n",
        "\n",
        "# evaluate 100\n",
        "scores, diff = [], []\n",
        "for _ in range(100):\n",
        "    sample = trainer.gen_sample()\n",
        "    pred = torch.max(classifier(sample), 1)\n",
        "    distr = torch.unique(pred[1], return_counts=True)[1].cpu().numpy()\n",
        "    diff.append(np.mean(abs((distr / np.sum(distr)) - 0.1)))\n",
        "    scores.append(np.mean(pred[0].detach().cpu().numpy()))\n",
        "\n",
        "print(\"Avg. score: \" + str(np.mean(scores)))\n",
        "print(\"Avg. dev. from uniform: \" + str(np.mean(diff)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU15Lom_v-7w",
        "colab_type": "text"
      },
      "source": [
        "## Different tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoOgJVvs4Ys4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_noise = False\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = False\n",
        "md_dim = 1\n",
        "md_k = 2\n",
        "\n",
        "stddev_layer = True\n",
        "\n",
        "label_smooth = False\n",
        "\n",
        "unroll_steps = 0\n",
        "\n",
        "for i in range(25):\n",
        "    stddev_trainer = Trainer()\n",
        "    s_G_loss, s_D_loss, s_stddev_f, s_stddev_r = stddev_trainer.train()\n",
        "\n",
        "    G_losses.append(s_G_loss)\n",
        "    D_losses.append(s_D_loss)\n",
        "    stddevs_f.append(s_stddev_f)\n",
        "    stddevs_r.append(s_stddev_r)\n",
        "\n",
        "    plot(s_G_loss, s_D_loss, s_stddev_f, s_stddev_r)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    show_sample(stddev_trainer.gen_sample())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "145pnPsm4xI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_noise = True\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = False\n",
        "md_dim = 1\n",
        "md_k = 2\n",
        "\n",
        "stddev_layer = False\n",
        "\n",
        "label_smooth = False\n",
        "\n",
        "unroll_steps = 0\n",
        "\n",
        "for _ in range(25):\n",
        "    noise_trainer = Trainer()\n",
        "    n_G_loss, n_D_loss, n_stddev_f, n_stddev_r = noise_trainer.train()\n",
        "\n",
        "    G_losses.append(n_G_loss)\n",
        "    D_losses.append(n_D_loss)\n",
        "    stddevs_f.append(n_stddev_f)\n",
        "    stddevs_r.append(n_stddev_r)\n",
        "\n",
        "    plot(n_G_loss, n_D_loss, n_stddev_f, n_stddev_r)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    show_sample(noise_trainer.gen_sample())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_7B_RVo4yXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_noise = False\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = True\n",
        "md_dim = 2\n",
        "md_k = 4\n",
        "stddev_layer = False\n",
        "\n",
        "label_smooth = False\n",
        "\n",
        "unroll_steps = 0\n",
        "\n",
        "for _ in range(25):\n",
        "    \n",
        "    mini_trainer = Trainer()\n",
        "    m_G_loss, m_D_loss, m_stddev_f, m_stddev_r = mini_trainer.train()\n",
        "\n",
        "    G_losses.append(m_G_loss)\n",
        "    D_losses.append(m_D_loss)\n",
        "    stddevs_f.append(m_stddev_f)\n",
        "    stddevs_r.append(m_stddev_r)\n",
        "\n",
        "    plot(m_G_loss, m_D_loss, m_stddev_f, m_stddev_r)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    show_sample(mini_trainer.gen_sample())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PrqAdMU444Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_noise = False\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = False\n",
        "md_dim = 1\n",
        "md_k = 2\n",
        "\n",
        "stddev_layer = False\n",
        "\n",
        "label_smooth = True\n",
        "\n",
        "unroll_steps = 0\n",
        "\n",
        "for _ in range(25):\n",
        "    smooth_trainer = Trainer()\n",
        "    sm_G_loss, sm_D_loss, sm_stddev_f, sm_stddev_r = smooth_trainer.train()\n",
        "\n",
        "    G_losses.append(sm_G_loss)\n",
        "    D_losses.append(sm_D_loss)\n",
        "    stddevs_f.append(sm_stddev_f)\n",
        "    stddevs_r.append(sm_stddev_r)\n",
        "\n",
        "    plot(sm_G_loss, sm_D_loss, sm_stddev_f, sm_stddev_r)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    show_sample(smooth_trainer.gen_sample())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoNHMZuL45ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_noise = False\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = False\n",
        "md_dim = 1\n",
        "md_k = 2\n",
        "\n",
        "stddev_layer = False\n",
        "\n",
        "label_smooth = False\n",
        "\n",
        "unroll_steps = 2\n",
        "\n",
        "# Warning: This one takes a long time. Consider making a sandwich.\n",
        "for _ in range(25):\n",
        "    unroll_trainer = Trainer()\n",
        "    u_G_loss, u_D_loss, u_stddev_f, u_stddev_r = unroll_trainer.train()\n",
        "\n",
        "    G_losses.append(u_G_loss)\n",
        "    D_losses.append(u_D_loss)\n",
        "    stddevs_f.append(u_stddev_f)\n",
        "    stddevs_r.append(u_stddev_r)\n",
        "\n",
        "    plot(u_G_loss, u_D_loss, u_stddev_f, u_stddev_r)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    show_sample(unroll_trainer.gen_sample())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzuWWdX7ebC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_noise = True\n",
        "annealing = 0.99\n",
        "\n",
        "mini_disc = True\n",
        "md_dim = 1\n",
        "md_k = 2\n",
        "\n",
        "stddev_layer = False\n",
        "\n",
        "label_smooth = True\n",
        "\n",
        "unroll_steps = 1\n",
        "\n",
        "# experimental trainer throwing all of the methods above on top of each other\n",
        "# rarely works as well as it should\n",
        "for _ in range(5):\n",
        "    all_trainer = Trainer()\n",
        "    a_G_loss, a_D_loss, a_stddev_f, a_stddev_r = all_trainer.train()\n",
        "\n",
        "    G_losses.append(a_G_loss)\n",
        "    D_losses.append(a_D_loss)\n",
        "    stddevs_f.append(a_stddev_f)\n",
        "    stddevs_r.append(a_stddev_r)\n",
        "\n",
        "    plot(a_G_loss, a_D_loss, a_stddev_f, a_stddev_r)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    show_sample(all_trainer.gen_sample())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVGhL8s6wKuc",
        "colab_type": "text"
      },
      "source": [
        "## Training Scoring Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vuApOnCwOPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.con = nn.Conv2d(nc, nc * 4, 4)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.con2 = nn.ConvTranspose2d(nc * 4, nc * 16, 4)\n",
        "        self.flat = nn.Linear(nc * 16 * 256, 100)\n",
        "\n",
        "        self.lin1 = nn.Linear(100, 75)\n",
        "        self.lin2 = nn.Linear(75, 10)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        x = self.pool(self.con(input))\n",
        "        x = self.pool(self.con2(x))\n",
        "        x = x.view(-1, 16 * 256)\n",
        "        x = F.relu(self.flat(x))\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwEhsJ3e1WtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = MNISTClassifier().to(device)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(2):\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = crit(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[%d, %d] loss: %.3f' %\n",
        "                  (epoch, i, loss.item()))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzuaq9E86TMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is designed for local use in Colab\n",
        "# If trying to use his elsewhere, do provide a more suitable method of saving.\n",
        "torch.save(net, './classifier')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpQFEjhi4Q3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores, diff = [], []\n",
        "\n",
        "# Testing the classifier on some training data\n",
        "for _ in range(100):\n",
        "    img, _ = next(iter(trainloader))\n",
        "\n",
        "    pred = torch.max(net(img.to(device)), 1)\n",
        "    distr = torch.unique(pred[1], return_counts=True)[1].cpu().numpy()\n",
        "    diff.append(abs((distr / np.sum(distr)) - 0.1))\n",
        "    scores.append(np.mean(pred[0].detach().cpu().numpy()))\n",
        "\n",
        "# It generally works!\n",
        "print(\"Avg. score: \" + str(np.mean(scores)))\n",
        "print(\"Avg. deviation from uniform: \" + str(np.mean(diff)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}